{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShayHav/MachineLearning/blob/main/Ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ex1 - Unsupervised learning**"
      ],
      "metadata": {
        "id": "djcj7z3isEoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Names and IDs\n",
        "\n",
        "1.   Shay Havivyan 315367995\n",
        "2.   Shahar Lankry 208600395\n"
      ],
      "metadata": {
        "id": "bbLvYfOMnaOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignemnt you will practice unsupervised methods we saw in class, and specifically running K-means and visualizing the data using PCA.\n",
        "\n",
        "In this assignment you will learn a few more things:\n",
        "\n",
        "1.   Load local files\n",
        "2.   Load data from Kaggle\n",
        "3. Use Scikit-learn K-means\n",
        "4. Use Scikit-learn PCA\n",
        "5. Some visulaization\n",
        "6. Evaluate the performance of the clustering using Elbow methods, Siouhette analysis and accuracy (as we have true labels)\n",
        "\n",
        "Note:\n",
        "* Read the complete task before implementing.\n",
        "* Reuse code, write functions."
      ],
      "metadata": {
        "id": "vpmlOOaKWUHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import All Packages\n",
        "Add all imports needed for this notebook to run"
      ],
      "metadata": {
        "id": "SmSdJm9coKpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here\n",
        "import sklearn as sk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ZAw8GGhYT1gJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Clustering And Dimension Reduction"
      ],
      "metadata": {
        "id": "y_-HIZ7Rnkfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you will cluster fake news using `k-means` and visualize the clustering using PCA."
      ],
      "metadata": {
        "id": "kQKN1XWKtGu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the Fake News Dataset from Kaggle**\n",
        "\n",
        "Navigate to https://www.kaggle.com. Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account) and select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "\n",
        "Then run the cell below and click the upload button to upload kaggle.json to your Colab runtime.\n",
        "\n",
        "After uploading the kaggle.json the fake news dataset will be copy to the enviroment in the '/content' directory. You will see the two files 'Fake.csv' and 'True.csv'.\n",
        "\n",
        "For more about the dataset you can read [here](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)."
      ],
      "metadata": {
        "id": "cjvrUWutYAQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# upload kaggle.json file using user prompt\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "\n",
        "# extract the files\n",
        "!unzip '/content/fake-and-real-news-dataset.zip'"
      ],
      "metadata": {
        "id": "pt0gDVr_YGvc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "57d57b35-5a90-4ea0-d8af-22daaa6c0619"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48551c73-8bed-4e11-a04a-252ae4e86566\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48551c73-8bed-4e11-a04a-252ae4e86566\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 68 bytes\n",
            "Downloading fake-and-real-news-dataset.zip to /content\n",
            " 98% 40.0M/41.0M [00:02<00:00, 32.7MB/s]\n",
            "100% 41.0M/41.0M [00:02<00:00, 20.4MB/s]\n",
            "Archive:  /content/fake-and-real-news-dataset.zip\n",
            "  inflating: Fake.csv                \n",
            "  inflating: True.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Read the csv files and create one single dataframe (5 points)\n",
        "\n",
        "*   Create a dataframe which is the join of the two files 'Fake.csv' and 'True.csv'.\n",
        "*   Extract the 'text' column from each dataframe (droping title, subject and date columns).\n",
        "*   Create a single dataframe containing a text column and a label column (fake=0, real=1)."
      ],
      "metadata": {
        "id": "sIT7GQbLo9-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here\n",
        "# Example for reading one file. It will work if the previous step was successful.\n",
        "\n",
        "fake_df = pd.read_csv('/content/Fake.csv')\n",
        "fake_df = fake_df[[\"text\"]].assign(label = 0)\n",
        "true_df = pd.read_csv('/content/True.csv')\n",
        "true_df = true_df[[\"text\"]].assign(label = 1)\n",
        "df = pd.concat([fake_df, true_df])\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "tYvY12rep0sO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b30cdf2-61b3-461e-f7ce-6a579639a8de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0      Donald Trump just couldn t wish all Americans ...      0\n",
            "1      House Intelligence Committee Chairman Devin Nu...      0\n",
            "2      On Friday, it was revealed that former Milwauk...      0\n",
            "3      On Christmas day, Donald Trump announced that ...      0\n",
            "4      Pope Francis used his annual Christmas Day mes...      0\n",
            "...                                                  ...    ...\n",
            "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...      1\n",
            "21413  LONDON (Reuters) - LexisNexis, a provider of l...      1\n",
            "21414  MINSK (Reuters) - In the shadow of disused Sov...      1\n",
            "21415  MOSCOW (Reuters) - Vatican Secretary of State ...      1\n",
            "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...      1\n",
            "\n",
            "[44898 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cluster the samples into 2 clusters (real and fake) (15 points)\n",
        "\n",
        "*   Generate TF-IDF features by applying the TfidfVectorizer preprocessor using 1000 features (`max_features=1000`) and remove English stop words.\n",
        "* Scale the data.\n",
        "*    Apply k-Means algorithm on the TF-IDF features using n_clusters=2."
      ],
      "metadata": {
        "id": "-Wp5qcYwqBxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "gIpApLQvqB-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Visualize using scatter plot (15 points)\n",
        "The data is high dimensional, so for visualization purpose, we will perform a dimensionality reduction using PCA.\n",
        "\n",
        "* Apply PCA\n",
        "*   Visualize the clustering in 2d using first two PCs.\n",
        "*   Visualize the clustering in 3d using first three PCs.\n",
        "\n",
        "Notes:\n",
        "\n",
        "*   In clustering visualization it's important to also visualize the centroids.\n",
        "* Visualize using matplotlib scatter function. It can be used to plot 2D or 3D scatter plots.\n",
        "* Color the points according to the true labels."
      ],
      "metadata": {
        "id": "d4hK3vpTtRTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "DUhtdsvGtlJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Performance evaluation (15 points)\n",
        "### How did our clustering perform against the real labels?\n",
        "\n",
        "We do have the true lables (fake/real), but we don't know which cluster correspond to which label. Therefore, we check the two options:\n",
        "\n",
        "*   Define cluster 1 as fake and cluster 2 as real. What is the accuracy?\n",
        "*   Define cluster 1 as true and cluster 2 as fake. What is the accuracy?"
      ],
      "metadata": {
        "id": "ELEMd4WFFTwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "hVVyjDIoJKeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Find optimal `k` (10 points)\n",
        "What is the best k for clustering?\n",
        "\n",
        "* Cluster using a range of `k` (up to 20) and compare the SSD and the Silhouette values for every k.\n",
        "* Plot SSD vs. k and Silhouette score vs. k.\n",
        "\n",
        "Notes:\n",
        "* You can get the SSD of a clustering using the `inertia_` attribute of the model.\n",
        "* Silhouette score using `silhouette_score` function from `sklearn.metrics`. This function accept the model and the data.\n",
        "* Computing Silhouette may takes long time. Estimate the Silhouette using a sample of 300 samples uisng the argument `sample_size=300`."
      ],
      "metadata": {
        "id": "Go3Jx05Wcsr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "DvGLxRHGWPop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. What is the optimal k for clustering? (10 points)\n",
        "\n",
        "* Explain.\n",
        "* If optimal k!=2 what can be a good explanation for this?"
      ],
      "metadata": {
        "id": "7WcPTIareSCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here"
      ],
      "metadata": {
        "id": "k3s2biB6uyVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. PCA then k-means (10 points)\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "This time, lets try to change the order.\n",
        "1. Generate TF-IDF 1000 features\n",
        "2. Run PCA (using all dimenssions)\n",
        "3. Run k-means\n",
        "4. Plot 2D and 3D scatter plots\n",
        "5. Estimate the accuracy according to true labels.\n",
        "6. Explain how is it compared to the previous approach, where clustering is performed w/o PCA.\n",
        "\n",
        "Note:\n",
        "* When you compute the PCA, don't limit to first PCs, but use all of them."
      ],
      "metadata": {
        "id": "g1Cuo2LNR3R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Answer \"\"\""
      ],
      "metadata": {
        "id": "NJf98Fx9WCce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST clustering (10 points)\n",
        "Redo the same analysis, but this time using the MNIST data set. Notice that this time there are actually 10 'true' clusters.\n",
        "\n",
        "1. What is the accuracy of the clustering when using k=10? \\\\\n",
        "   * When for computing the accuracy of 10 classes, first you need to define what is the label of each cluster. Do that by majority votes. In theory, you may get two or more clusters with the same labels. We will ignore that for now. Meaning that you don't need to bother in case there are two clusters with the same label according to the majority vote. Simply count what is the fraction of 'other' digits in each cluster.\n",
        "2. What is the optimal k?\n",
        "3. How do the results changes if you first run PCA?"
      ],
      "metadata": {
        "id": "JiYp9sP0apoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute projection (10 points)\n",
        "**Notice:** No code required in this section.\n",
        "\n",
        "Given the next PCA projection matrix\n",
        "$\\begin{pmatrix}\n",
        "1 & 1\\\\\n",
        "2 & -1\n",
        "\\end{pmatrix}$\n",
        "And the correspoding egienvalues\n",
        "$(5, -1)$\n",
        "\n",
        "Compute the projection to one dimension of the next two vectors:\n",
        "\\begin{pmatrix}\n",
        "1 & 3\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "3 & 7\n",
        "\\end{pmatrix}"
      ],
      "metadata": {
        "id": "3ft1JUt39-4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer here and show your computations (no code)"
      ],
      "metadata": {
        "id": "VCQQ0Jzxapo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answers here*"
      ],
      "metadata": {
        "id": "Fzv4mYUkvqMB"
      }
    }
  ]
}